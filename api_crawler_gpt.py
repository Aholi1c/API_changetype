import sys
import os
import json
import csv
import time
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from visit_gpt4o_fixed import VisitGPT4o  

# -------------------------- æ ¸å¿ƒé…ç½® --------------------------
INPUT_CSV = "input.csv"       # URLçš„è¾“å…¥æ–‡ä»¶
OUTPUT_CSV = "output.csv"  # æœ€ç»ˆç»“æœè¾“å‡ºæ–‡ä»¶ - ä¿®æ”¹æ–‡ä»¶å
TEMP_CSV = "api_crawl_temp_gpt5.csv"       # ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºæ–­ç‚¹ç»­çˆ¬ï¼Œé¿å…æ•°æ®ä¸¢å¤±ï¼‰ - ä¿®æ”¹æ–‡ä»¶å
# çˆ¬å–ç­–ç•¥é…ç½®ï¼ˆé’ˆå¯¹å¤§æ•°é‡URLä¼˜åŒ–ï¼‰
MAX_RETRIES = 2                       # æ¯ä¸ªURLæœ€å¤šé‡è¯•2æ¬¡ï¼ˆå‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
RETRY_DELAY = 2                       # é‡è¯•é—´éš”2ç§’ï¼ˆç»™APIæ›´å¤šæ—¶é—´ï¼‰
MAX_WORKERS = 3                       # çº¿ç¨‹æ•°ï¼šå‡å°‘åˆ°3ï¼ˆé¿å…APIé™åˆ¶ï¼‰
BATCH_SIZE = 10                       # æ¯çˆ¬å–10æ¡URLï¼ŒåŒæ­¥ä¸€æ¬¡ä¸´æ—¶æ–‡ä»¶ï¼ˆæ›´é¢‘ç¹ä¿å­˜ï¼‰

# å…¨å±€é”ï¼ˆé¿å…å¤šçº¿ç¨‹å†™å…¥CSVå†²çªï¼‰
csv_lock = threading.Lock()


# -------------------------- 1. åŸºç¡€å·¥å…·å‡½æ•° --------------------------
def load_urls_from_csv(csv_file, temp_file=TEMP_CSV):
    """
    åŠ è½½URLåˆ—è¡¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼š
    - è·³è¿‡å·²çˆ¬å–æˆåŠŸçš„URL
    - è®°å½•æœªçˆ¬å–/çˆ¬å–å¤±è´¥çš„URLï¼Œç»§ç»­å¤„ç†
    """
    # ç¬¬ä¸€æ­¥ï¼šè¯»å–å·²çˆ¬å–æˆåŠŸçš„URLï¼ˆä»ä¸´æ—¶æ–‡ä»¶ï¼‰
    completed_urls = set()
    if os.path.exists(temp_file):
        with open(temp_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            if "url" in reader.fieldnames and "crawl_status" in reader.fieldnames:
                for row in reader:
                    if row["crawl_status"] == "success":
                        completed_urls.add(row["url"].strip())
        print(f"ğŸ” å‘ç°ä¸´æ—¶æ–‡ä»¶ï¼Œå·²çˆ¬å–æˆåŠŸ {len(completed_urls)} æ¡URLï¼Œå°†è·³è¿‡è¿™äº›URL")

    # ç¬¬äºŒæ­¥ï¼šè¯»å–è¾“å…¥CSVçš„æ‰€æœ‰URLï¼Œè¿‡æ»¤å·²å®Œæˆçš„
    all_urls = []
    try:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            if "url" not in reader.fieldnames:
                raise ValueError("è¾“å…¥CSVå¿…é¡»åŒ…å«'url'è¡¨å¤´")

            for row_num, row in enumerate(reader, 2):  # è¡Œå·ä»2å¼€å§‹ï¼ˆè¡¨å¤´ä¸º1ï¼‰
                url = row["url"].strip()
                if url and url not in completed_urls:  # è·³è¿‡ç©ºURLå’Œå·²å®Œæˆçš„URL
                    all_urls.append({
                        "url": url,
                        "original_row_num": row_num  # è®°å½•åŸå§‹è¡Œå·ï¼Œä¾¿äºæ ¸å¯¹
                    })

        total_input = len(all_urls) + len(completed_urls)
        print(f"âœ… ä»è¾“å…¥CSVåŠ è½½å®Œæˆï¼šæ€»è®¡ {total_input} æ¡URLï¼Œå¾…çˆ¬å– {len(all_urls)} æ¡ï¼Œå·²å®Œæˆ {len(completed_urls)} æ¡")
        return all_urls

    except FileNotFoundError:
        print(f"âŒ è¾“å…¥CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{csv_file}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ è¯»å–è¾“å…¥CSVå¤±è´¥ï¼š{str(e)}")
        sys.exit(1)


def init_temp_csv(temp_file=TEMP_CSV):
    """åˆå§‹åŒ–ä¸´æ—¶CSVæ–‡ä»¶ï¼ˆç”¨äºæ–­ç‚¹ç»­çˆ¬ï¼‰"""
    if not os.path.exists(temp_file):
        with open(temp_file, "w", newline="", encoding="utf-8") as f:
            csv_columns = get_csv_columns()
            writer = csv.DictWriter(f, fieldnames=csv_columns, restval="")
            writer.writeheader()
    return temp_file


def get_csv_columns():
    """å®šä¹‰CSVè¾“å‡ºå­—æ®µï¼ˆå›ºå®šå­—æ®µé¡ºåºï¼Œé¿å…é”™ä¹±ï¼‰"""
    return [
        # åŸºç¡€å®šä½ä¿¡æ¯
        "original_row_num", "url", "crawl_time", "crawl_status", "error_msg",
        # APIæ ¸å¿ƒä¿¡æ¯
        "api", "package", "language",
        # APIå˜æ›´ä¿¡æ¯
        "deprecated_in", "removed_in", "replaced_by", "change_type", "reason",
        # æ¥æºä¿¡æ¯
        "source"
    ]


# -------------------------- 2. çˆ¬å–æ ¸å¿ƒå‡½æ•°ï¼ˆæ”¯æŒå¤šçº¿ç¨‹ï¼‰ --------------------------
def crawl_single_api(url, original_row_num):
    """å•URLçˆ¬å–å‡½æ•°ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œè¿”å›çˆ¬å–ç»“æœå­—å…¸ï¼‰ - ä½¿ç”¨GPT-5"""
    result = {
        "original_row_num": original_row_num,
        "url": url,
        "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "crawl_status": "failed",
        "error_msg": ""
    }

    for retry in range(MAX_RETRIES):
        try:
            visit_tool = VisitGPT4o()  # ä½¿ç”¨VisitGPT4oæ›¿ä»£Visit
            crawl_params = json.dumps({
                "url": url,
                "goal": "åªæå–é¡µé¢ä¸­æ˜ç¡®å­˜åœ¨çš„APIå˜æ›´ä¿¡æ¯ã€‚å¦‚æœé¡µé¢æ²¡æœ‰æ˜ç¡®çš„å˜æ›´è¯´æ˜ï¼Œchange_typeå’Œreasonå¿…é¡»ä¸ºç©ºã€‚ä¸¥ç¦ç¼–é€ æˆ–æ¨æ–­ä»»ä½•ä¿¡æ¯ã€‚"
            })
            result_str = visit_tool.call(crawl_params)

            if not result_str.strip():
                raise ValueError("çˆ¬å–ç»“æœä¸ºç©ºå­—ç¬¦ä¸²")

            # åˆå¹¶çˆ¬å–åˆ°çš„APIä¿¡æ¯
            api_data = json.loads(result_str)
            result.update(api_data)
            result["crawl_status"] = "success"
            result["error_msg"] = ""
            return result  # çˆ¬å–æˆåŠŸï¼Œç›´æ¥è¿”å›

        except Exception as e:
            error_msg = f"ç¬¬{retry+1}æ¬¡é‡è¯•å¤±è´¥ï¼š{str(e)}"
            result["error_msg"] = error_msg
            if retry < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)  # æœªåˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç­‰å¾…åé‡è¯•

    # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œè¿”å›å¤±è´¥ç»“æœ
    result["error_msg"] = f"è¶…è¿‡{MAX_RETRIES}æ¬¡é‡è¯•ï¼š{result['error_msg']}"
    return result


def write_result_to_csv(result, csv_file, lock):
    """çº¿ç¨‹å®‰å…¨çš„CSVå†™å…¥å‡½æ•°ï¼ˆé€šè¿‡é”é¿å…å¤šçº¿ç¨‹å†™å…¥å†²çªï¼‰"""
    with lock:
        with open(csv_file, "a", newline="", encoding="utf-8") as f:
            csv_columns = get_csv_columns()
            writer = csv.DictWriter(f, fieldnames=csv_columns, restval="")
            # è¿‡æ»¤æ‰ä¸åœ¨å­—æ®µåˆ—è¡¨ä¸­çš„é”®ï¼ˆé¿å…å¤šä½™å­—æ®µå¯¼è‡´æŠ¥é”™ï¼‰
            filtered_result = {k: result.get(k, "") for k in csv_columns}
            writer.writerow(filtered_result)


# -------------------------- 3. æ‰¹é‡çˆ¬å–ä¸»é€»è¾‘ï¼ˆé’ˆå¯¹482æ¡URLä¼˜åŒ–ï¼‰ --------------------------
def batch_crawl_large_scale(input_csv, output_csv, temp_csv):
    # 1. åˆå§‹åŒ–ï¼šåŠ è½½å¾…çˆ¬URLã€åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶
    all_urls = load_urls_from_csv(input_csv, temp_csv)
    if not all_urls:
        print("ğŸ‰ æ‰€æœ‰URLå·²çˆ¬å–å®Œæˆï¼Œæ— éœ€ç»§ç»­æ‰§è¡Œ")
        # å°†ä¸´æ—¶æ–‡ä»¶é‡å‘½åä¸ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if os.path.exists(temp_csv) and not os.path.exists(output_csv):
            os.rename(temp_csv, output_csv)
        sys.exit(0)

    init_temp_csv(temp_csv)  # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶å­˜åœ¨ä¸”è¡¨å¤´æ­£ç¡®

    # 2. åˆå§‹åŒ–è¿›åº¦ç»Ÿè®¡
    total_to_crawl = len(all_urls)
    completed_count = 0
    success_count = 0
    fail_count = 0
    start_time = datetime.now()

    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–ï¼ˆä½¿ç”¨GPT-5ï¼‰ï¼š{start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š é…ç½®ï¼šçº¿ç¨‹æ•°={MAX_WORKERS}ï¼Œé‡è¯•æ¬¡æ•°={MAX_RETRIES}ï¼Œæ¯{MAX_WORKERS}æ¡åŒæ­¥ä¸´æ—¶æ–‡ä»¶")
    print(f"â³ é¢„è®¡è€—æ—¶ï¼š{total_to_crawl / MAX_WORKERS * 2:.1f} ç§’ï¼ˆä¼°ç®—ï¼‰\n")

    # 3. å¤šçº¿ç¨‹æ‰¹é‡çˆ¬å–ï¼ˆåˆ†æ‰¹æ¬¡å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»ºè¿‡å¤šçº¿ç¨‹ï¼‰
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰çˆ¬å–ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
        future_tasks = {
            executor.submit(crawl_single_api, url_info["url"], url_info["original_row_num"]):
            url_info for url_info in all_urls
        }

        # å®æ—¶å¤„ç†å®Œæˆçš„ä»»åŠ¡ï¼Œæ›´æ–°è¿›åº¦
        for future in as_completed(future_tasks):
            url_info = future_tasks[future]
            url = url_info["url"]
            completed_count += 1

            try:
                # è·å–çˆ¬å–ç»“æœ
                result = future.result(timeout=60)  # è¶…æ—¶æ—¶é—´60ç§’ï¼ˆé¿å…çº¿ç¨‹æŒ‚èµ·ï¼‰
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                write_result_to_csv(result, temp_csv, csv_lock)
                # æ›´æ–°ç»Ÿè®¡
                if result["crawl_status"] == "success":
                    success_count += 1
                    print(f"âœ… [{completed_count}/{total_to_crawl}] æˆåŠŸï¼š{url}")
                else:
                    fail_count += 1
                    print(f"âŒ [{completed_count}/{total_to_crawl}] å¤±è´¥ï¼š{url}ï¼ˆ{result['error_msg'][:50]}...ï¼‰")

                # æ¯çˆ¬å–BATCH_SIZEæ¡ï¼Œæ‰“å°ä¸€æ¬¡è¿›åº¦æ±‡æ€»
                if completed_count % BATCH_SIZE == 0 or completed_count == total_to_crawl:
                    elapsed_time = (datetime.now() - start_time).total_seconds()
                    avg_time_per_url = elapsed_time / completed_count if completed_count > 0 else 0
                    remaining_time = avg_time_per_url * (total_to_crawl - completed_count)
                    print(f"\nğŸ“ˆ è¿›åº¦æ±‡æ€»ï¼šå·²å®Œæˆ{completed_count}/{total_to_crawl}ï¼ˆæˆåŠŸ{success_count}ï¼Œå¤±è´¥{fail_count}ï¼‰")
                    print(f"â±ï¸  å·²è€—æ—¶ï¼š{elapsed_time:.1f}ç§’ï¼Œé¢„è®¡å‰©ä½™ï¼š{remaining_time:.1f}ç§’\n")

            except Exception as e:
                # æ•è·çº¿ç¨‹æ‰§è¡Œä¸­çš„å¼‚å¸¸ï¼ˆå¦‚è¶…æ—¶ã€æœªçŸ¥é”™è¯¯ï¼‰
                fail_count += 1
                error_result = {
                    "original_row_num": url_info["original_row_num"],
                    "url": url,
                    "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "crawl_status": "failed",
                    "error_msg": f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}"
                }
                write_result_to_csv(error_result, temp_csv, csv_lock)
                print(f"âŒ [{completed_count}/{total_to_crawl}] å¼‚å¸¸ï¼š{url}ï¼ˆ{str(e)[:50]}...ï¼‰")

    # 4. çˆ¬å–å®Œæˆï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š + åˆå¹¶ä¸´æ—¶æ–‡ä»¶åˆ°è¾“å‡ºæ–‡ä»¶
    end_time = datetime.now()
    total_elapsed = (end_time - start_time).total_seconds()

    # å°†ä¸´æ—¶æ–‡ä»¶é‡å‘½åä¸ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶ï¼ˆè¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶ï¼‰
    if os.path.exists(temp_csv):
        if os.path.exists(output_csv):
            os.remove(output_csv)  # åˆ é™¤æ—§çš„è¾“å‡ºæ–‡ä»¶
        os.rename(temp_csv, output_csv)
        print(f"ğŸ“ ä¸´æ—¶æ–‡ä»¶å·²åˆå¹¶ä¸ºæœ€ç»ˆç»“æœï¼š{os.path.abspath(output_csv)}")

    # æ‰“å°æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰¹é‡çˆ¬å–ä»»åŠ¡å®Œæˆï¼ˆä½¿ç”¨GPT-5ï¼‰")
    print("=" * 60)
    print(f"ğŸ“Š æ€»ç»Ÿè®¡ï¼š")
    print(f"   - è¾“å…¥URLæ€»æ•°ï¼š{len(all_urls) + success_count + fail_count}")
    print(f"   - å¾…çˆ¬URLæ•°ï¼š{total_to_crawl}")
    print(f"   - æˆåŠŸæ•°ï¼š{success_count}")
    print(f"   - å¤±è´¥æ•°ï¼š{fail_count}")
    print(f"   - æˆåŠŸç‡ï¼š{success_count / total_to_crawl * 100:.1f}%" if total_to_crawl > 0 else "0%")
    print(f"â±ï¸  è€—æ—¶ï¼š{total_elapsed // 60:.0f}åˆ†{total_elapsed % 60:.1f}ç§’")
    print(f"ğŸ“„ ç»“æœæ–‡ä»¶ï¼š{os.path.abspath(output_csv)}")
    print("=" * 60)


# -------------------------- 4. æ‰§è¡Œå…¥å£ --------------------------
if __name__ == "__main__":
    # 1. æ·»åŠ é¡¹ç›®è·¯å¾„ï¼ˆç¡®ä¿èƒ½å¯¼å…¥VisitGPT5ç±»ï¼‰
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(current_dir)

    # 2. æ£€æŸ¥ä¾èµ–æ–‡ä»¶
    if not os.path.exists(INPUT_CSV):
        print(f"âŒ è¾“å…¥CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{INPUT_CSV}")
        sys.exit(1)

    # 3. å¯åŠ¨å¤§è§„æ¨¡æ‰¹é‡çˆ¬å–ï¼ˆä½¿ç”¨GPT-5ï¼‰
    print("ğŸ¤– ä½¿ç”¨GPT-5è¿›è¡ŒAPIä¿¡æ¯æå–")
    batch_crawl_large_scale(
        input_csv=INPUT_CSV,
        output_csv=OUTPUT_CSV,
        temp_csv=TEMP_CSV
    )