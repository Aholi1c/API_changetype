#!/usr/bin/env python3
"""
Enhanced API Crawler with Smart Identification
åŸºäºWebAgent/api_crawler.pyçš„æ¶æ„ï¼Œå¢åŠ äº†æ™ºèƒ½APIè¯†åˆ«åŠŸèƒ½
ä¸“é—¨è§£å†³URLä¸APIä¸åŒ¹é…çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒä¸api_crawler.pyç›¸åŒçš„è¾“å‡ºæ ¼å¼
"""

import sys
import os
import json
import csv
import time
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from openai import AzureOpenAI

# -------------------------- æ ¸å¿ƒé…ç½® --------------------------
INPUT_CSV = "pre_data.csv"  # URLçš„è¾“å…¥æ–‡ä»¶ï¼ˆå¯ä¿®æ”¹ä¸ºReactç­‰ï¼‰
OUTPUT_CSV = "output.csv"  # æœ€ç»ˆç»“æœè¾“å‡ºæ–‡ä»¶
TEMP_CSV = "enhanced_api_crawl_temp.csv"  # ä¸´æ—¶æ–‡ä»¶ï¼ˆç”¨äºæ–­ç‚¹ç»­çˆ¬ï¼‰

# çˆ¬å–ç­–ç•¥é…ç½®
MAX_RETRIES = 3
RETRY_DELAY = 1
MAX_WORKERS = 8
BATCH_SIZE = 50

# Azure OpenAIé…ç½®
AZURE_ENDPOINT = "https://test-openai-startup.openai.azure.com/"
AZURE_DEPLOYMENT = "gpt-4o"
AZURE_API_KEY = "xxx"

# å…¨å±€é”ï¼ˆé¿å…å¤šçº¿ç¨‹å†™å…¥CSVå†²çªï¼‰
csv_lock = threading.Lock()


class EnhancedAPICrawler:
    def __init__(self):
        # åˆå§‹åŒ–Azure OpenAIå®¢æˆ·ç«¯
        try:
            self.client = AzureOpenAI(
                azure_endpoint=AZURE_ENDPOINT,
                api_key=AZURE_API_KEY,
                api_version="2024-02-15-preview"
            )
            self.use_gpt = True
            print("âœ… GPT-4o APIå·²å¯ç”¨ - æ™ºèƒ½APIè¯†åˆ«æ¨¡å¼")
        except Exception as e:
            print(f"âš ï¸ GPT-4o APIåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸºç¡€è¯†åˆ«")
            self.use_gpt = False

        # åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯
        import requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def extract_api_from_url(self, url: str) -> Dict[str, str]:
        """ä»URLä¸­æå–APIä¿¡æ¯çš„å¤šç§ç­–ç•¥"""
        result = {
            'api_name': '',
            'confidence': 0.0,
            'method': 'none'
        }

        # è§£æURLç»“æ„
        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')

        # ç­–ç•¥1: Fragment/Hashè¯†åˆ«
        if parsed.fragment:
            api_from_fragment = self._extract_from_fragment(parsed.fragment)
            if api_from_fragment:
                result['api_name'] = api_from_fragment
                result['confidence'] = 0.9
                result['method'] = 'fragment'
                return result

        # ç­–ç•¥2: URLè·¯å¾„è¯†åˆ«
        api_from_path = self._extract_from_path(path_parts, parsed.netloc)
        if api_from_path:
            result['api_name'] = api_from_path
            result['confidence'] = 0.8
            result['method'] = 'path'
            return result

        # ç­–ç•¥3: æŸ¥è¯¢å‚æ•°è¯†åˆ«
        if parsed.query:
            api_from_query = self._extract_from_query(parsed.query)
            if api_from_query:
                result['api_name'] = api_from_query
                result['confidence'] = 0.6
                result['method'] = 'query'
                return result

        return result

    def _extract_from_fragment(self, fragment: str) -> str:
        """ä»URL fragmentä¸­æå–APIåç§°"""
        clean_fragment = fragment

        # å¤„ç†æ–‡æ¡£çš„fragmentæ ¼å¼
        if '#' in fragment:
            clean_fragment = fragment.split('#')[-1]

        # ç§»é™¤å¸¸è§çš„å‰ç¼€
        prefixes_to_remove = ['_', '-', '/', 'api-', 'function-', 'method-']
        for prefix in prefixes_to_remove:
            if clean_fragment.startswith(prefix):
                clean_fragment = clean_fragment[len(prefix):]

        # æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„æ ‡è¯†ç¬¦
        match = re.match(r'^([a-zA-Z][\w\.]*)', clean_fragment)
        if match:
            api_name = match.group(1)
            # æ·»åŠ _å‰ç¼€ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
            if not api_name.startswith('_') and len(api_name) > 1:
                return f"_{api_name}"
            return api_name

        return ''

    def _extract_from_path(self, path_parts: List[str], domain: str) -> str:
        """ä»URLè·¯å¾„ä¸­æå–APIåç§°"""
        if not path_parts:
            return ''

        # Reactæ–‡æ¡£çš„ç‰¹æ®Šå¤„ç†
        if 'react.dev' in domain or 'reactjs.org' in domain:
            for part in reversed(path_parts):
                if part and not part in ['reference', 'docs', 'api', 'hooks']:
                    return part

        # Lodashæ–‡æ¡£çš„ç‰¹æ®Šå¤„ç†
        if 'lodash.com' in domain:
            for part in reversed(path_parts):
                if part and part not in ['docs', 'api']:
                    # Lodashå‡½æ•°é€šå¸¸æ˜¯_.å¼€å¤´æˆ–è€…çº¯å‡½æ•°å
                    if not part.startswith('_'):
                        return f"_{part}"
                    return part

        #å¯æ·»åŠ å…¶ä»–æ–‡æ¡£çš„ç‰¹æ®Šå¤„ç†

        # é€šç”¨è·¯å¾„æå–
        for part in reversed(path_parts):
            if part and len(part) > 1 and not part in ['docs', 'api', 'reference', 'v1', 'v2', 'v3']:
                return part

        return ''

    def _extract_from_query(self, query: str) -> str:
        """ä»æŸ¥è¯¢å‚æ•°ä¸­æå–APIåç§°"""
        from urllib.parse import parse_qs
        params = parse_qs(query)

        # å¸¸è§çš„APIå‚æ•°å
        api_param_names = ['api', 'function', 'method', 'func', 'name']

        for param_name in api_param_names:
            if param_name in params and params[param_name]:
                return params[param_name][0]

        return ''

    def crawl_page_content(self, url: str) -> Dict[str, Any]:
        """çˆ¬å–é¡µé¢å†…å®¹å¹¶æå–ç»“æ„åŒ–ä¿¡æ¯"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–é¡µé¢æ ‡é¢˜
            title = ''
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text().strip()

            # æå–æ‰€æœ‰æ ‡é¢˜
            headings = []
            for h1 in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                headings.append({
                    'level': int(h1.name[1]),
                    'text': h1.get_text().strip(),
                    'id': h1.get('id', '')
                })

            # æå–é¡µé¢ä¸»è¦æ–‡æœ¬å†…å®¹
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()

            main_content = soup.get_text(separator='\n', strip=True)

            return {
                'url': url,
                'title': title,
                'headings': headings,
                'content': main_content,
                'status': 'success'
            }

        except Exception as e:
            return {
                'url': url,
                'status': 'failed',
                'error': str(e)
            }

    def identify_target_api_with_gpt(self, url: str, api_from_url: str, page_content: Dict) -> Dict[str, Any]:
        """ä½¿ç”¨GPT-4oæ™ºèƒ½è¯†åˆ«ç›®æ ‡API"""
        if not self.use_gpt or page_content.get('status') != 'success':
            return self._fallback_identification(api_from_url, page_content)

        # å‡†å¤‡æç¤ºä¿¡æ¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªAPIæ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹URLå’Œé¡µé¢å†…å®¹ï¼Œè¯†åˆ«å‡ºè¿™ä¸ªURLä¸»è¦å¯¹åº”çš„æ˜¯å“ªä¸ªAPIã€‚

URLä¿¡æ¯:
- å®Œæ•´URL: {url}
- ä»URLè§£æçš„API: {api_from_url}

é¡µé¢ä¿¡æ¯:
- æ ‡é¢˜: {page_content.get('title', '')}
- ä¸»è¦æ ‡é¢˜: {json.dumps(page_content.get('headings', [])[:3], ensure_ascii=False)}

åˆ†æè§„åˆ™:
1. æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦æ˜ç¡®æåˆ°äº†æŸä¸ªAPIåç§°
2. æ£€æŸ¥ä¸»è¦æ ‡é¢˜(h1, h2)ä¸­æ˜¯å¦åŒ…å«APIåç§°
3. è€ƒè™‘URLè§£æçš„ç»“æœï¼Œä½†ä»¥é¡µé¢å®é™…å†…å®¹ä¸ºå‡†
4. å¯¹äºReactæ–‡æ¡£ï¼ŒAPIåç§°é€šå¸¸æ˜¯Hookåç§°æˆ–ç»„ä»¶åç§°
5. å¯¹äºLodashæ–‡æ¡£ï¼ŒAPIåç§°é€šå¸¸æ˜¯_å¼€å¤´çš„å‡½æ•°å

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœ:
{{
    "target_api": "è¯†åˆ«å‡ºçš„ä¸»è¦APIåç§°",
    "package": "å¯¹åº”çš„åŒ…åï¼ˆreact/lodashç­‰ï¼‰",
    "language": "ç¼–ç¨‹è¯­è¨€ï¼ˆJavaScriptç­‰ï¼‰",
    "deprecated_in": "å¼ƒç”¨ç‰ˆæœ¬ï¼ˆå¦‚æœé€‚ç”¨ï¼‰",
    "removed_in": "ç§»é™¤ç‰ˆæœ¬ï¼ˆå¦‚æœé€‚ç”¨ï¼‰",
    "replaced_by": "æ›¿ä»£APIï¼ˆå¦‚æœé€‚ç”¨ï¼‰",
    "change_type": "å˜æ›´ç±»å‹",
    "reason": "å˜æ›´åŸå› ",
    "source": "æ¥æºé“¾æ¥",
    "confidence": 0.9,
    "evidence": "åˆ¤æ–­ä¾æ®çš„è¯¦ç»†è¯´æ˜"
}}

æ³¨æ„ï¼š
- target_apiåº”è¯¥æ˜¯å…·ä½“çš„APIåç§°
- confidenceæ˜¯0-1ä¹‹é—´çš„ç½®ä¿¡åº¦åˆ†æ•°
- å¦‚æœæ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå­—æ®µç•™ç©º"""

        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„APIæ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»URLå’Œé¡µé¢å†…å®¹ä¸­è¯†åˆ«ç›®æ ‡APIä¿¡æ¯ã€‚"},
                {"role": "user", "content": prompt}
            ]

            response = self.client.chat.completions.create(
                model=AZURE_DEPLOYMENT,
                messages=messages,
                temperature=0.1,
                max_tokens=1500
            )

            result = json.loads(response.choices[0].message.content.strip())
            print(f"GPTè¯†åˆ«ç»“æœ: {result.get('target_api', 'N/A')} (ç½®ä¿¡åº¦: {result.get('confidence', 0):.2f})")
            return result

        except Exception as e:
            print(f"GPTè¯†åˆ«å¤±è´¥: {e}ï¼Œä½¿ç”¨å›é€€æ–¹æ³•")
            return self._fallback_identification(api_from_url, page_content)

    def _fallback_identification(self, api_from_url: str, page_content: Dict) -> Dict[str, Any]:
        """å›é€€è¯†åˆ«æ–¹æ³•ï¼ˆä¸ä½¿ç”¨GPTï¼‰"""
        title = page_content.get('title', '')
        api_from_title = self._extract_api_from_text(title)

        # ç¡®å®šåŒ…å
        package = ''
        if 'react' in title.lower() or 'react' in str(page_content.get('url', '')):
            package = 'react'
        elif 'lodash' in title.lower() or 'lodash' in str(page_content.get('url', '')):
            package = 'lodash'

        # ç¡®å®šAPIåç§°
        target_api = api_from_title if api_from_title else api_from_url

        return {
            'target_api': target_api,
            'package': package,
            'language': 'JavaScript',
            'deprecated_in': '',
            'removed_in': '',
            'replaced_by': '',
            'change_type': '',
            'reason': '',
            'source': page_content.get('url', ''),
            'confidence': 0.6,
            'evidence': f"åŸºäºURLè§£æå’Œé¡µé¢æ ‡é¢˜åˆ†æ: {title}"
        }

    def _extract_api_from_text(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–APIåç§°"""
        if not text:
            return ''

        # å¸¸è§çš„APIæ¨¡å¼
        patterns = [
            r'_([a-zA-Z][\w]*)',  # Lodashæ¨¡å¼: _.functionName
            r'use([A-Z]\w*)',     # React Hookæ¨¡å¼: useHookName
            r'react\.([A-Z]\w*)', # React APIæ¨¡å¼: React.Component
            r'\b([A-Z]\w*)\b',    # ç»„ä»¶æ¨¡å¼: ComponentName
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                match = matches[0]
                if pattern == patterns[0]:  # Lodash
                    return f"_{match}"
                elif pattern == patterns[1]:  # React Hook
                    return f"use{match}"
                elif pattern == patterns[2]:  # React API
                    return f"react.{match}"
                else:
                    return match

        return ''

    def crawl_single_api(self, url: str, original_row_num: int) -> Dict[str, str]:
        """å•URLçˆ¬å–å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼ŒåŒ…å«æ™ºèƒ½APIè¯†åˆ«ï¼‰"""
        result = {
            "original_row_num": original_row_num,
            "url": url,
            "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "crawl_status": "failed",
            "error_msg": ""
        }

        for retry in range(MAX_RETRIES):
            try:
                print(f"æ­£åœ¨å¤„ç†: {url} (å°è¯• {retry+1}/{MAX_RETRIES})")

                # ç¬¬ä¸€æ­¥ï¼šä»URLè§£æAPI
                url_info = self.extract_api_from_url(url)
                api_from_url = url_info.get('api_name', '')

                # ç¬¬äºŒæ­¥ï¼šçˆ¬å–é¡µé¢å†…å®¹
                page_content = self.crawl_page_content(url)
                if page_content.get('status') != 'success':
                    raise ValueError(f"é¡µé¢çˆ¬å–å¤±è´¥: {page_content.get('error', 'Unknown error')}")

                # ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨GPTæ™ºèƒ½è¯†åˆ«APIä¿¡æ¯
                api_data = self.identify_target_api_with_gpt(url, api_from_url, page_content)

                # åˆå¹¶ç»“æœåˆ°è¾“å‡ºæ ¼å¼
                result.update({
                    "api": api_data.get('target_api', api_from_url),
                    "package": api_data.get('package', ''),
                    "language": api_data.get('language', 'JavaScript'),
                    "deprecated_in": api_data.get('deprecated_in', ''),
                    "removed_in": api_data.get('removed_in', ''),
                    "replaced_by": api_data.get('replaced_by', ''),
                    "change_type": api_data.get('change_type', ''),
                    "reason": api_data.get('reason', ''),
                    "source": api_data.get('source', url),
                    "crawl_status": "success",
                    "error_msg": ""
                })

                print(f"âœ… æˆåŠŸè¯†åˆ«API: {result['api']} (ç½®ä¿¡åº¦: {api_data.get('confidence', 0):.2f})")
                return result

            except Exception as e:
                error_msg = f"ç¬¬{retry+1}æ¬¡é‡è¯•å¤±è´¥ï¼š{str(e)}"
                result["error_msg"] = error_msg
                if retry < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY)

        # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œè¿”å›å¤±è´¥ç»“æœ
        result["error_msg"] = f"è¶…è¿‡{MAX_RETRIES}æ¬¡é‡è¯•ï¼š{result['error_msg']}"
        result["api"] = url_info.get('api_name', 'unknown')
        return result


# -------------------------- åŸºç¡€å·¥å…·å‡½æ•° --------------------------
def load_urls_from_csv(csv_file, temp_file=TEMP_CSV):
    """åŠ è½½URLåˆ—è¡¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­çˆ¬"""
    completed_urls = set()
    if os.path.exists(temp_file):
        with open(temp_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            if "url" in reader.fieldnames and "crawl_status" in reader.fieldnames:
                for row in reader:
                    if row["crawl_status"] == "success":
                        completed_urls.add(row["url"].strip())
        print(f"ğŸ” å‘ç°ä¸´æ—¶æ–‡ä»¶ï¼Œå·²çˆ¬å–æˆåŠŸ {len(completed_urls)} æ¡URLï¼Œå°†è·³è¿‡è¿™äº›URL")

    # è¯»å–è¾“å…¥CSVçš„æ‰€æœ‰URLï¼Œè¿‡æ»¤å·²å®Œæˆçš„
    all_urls = []
    try:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            if "url" not in reader.fieldnames:
                raise ValueError("è¾“å…¥CSVå¿…é¡»åŒ…å«'url'è¡¨å¤´")

            for row_num, row in enumerate(reader, 2):
                url = row["url"].strip()
                if url and url not in completed_urls:
                    all_urls.append({
                        "url": url,
                        "original_row_num": row_num
                    })

        total_input = len(all_urls) + len(completed_urls)
        print(f"âœ… ä»è¾“å…¥CSVåŠ è½½å®Œæˆï¼šæ€»è®¡ {total_input} æ¡URLï¼Œå¾…çˆ¬å– {len(all_urls)} æ¡ï¼Œå·²å®Œæˆ {len(completed_urls)} æ¡")
        return all_urls

    except FileNotFoundError:
        print(f"âŒ è¾“å…¥CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{csv_file}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ è¯»å–è¾“å…¥CSVå¤±è´¥ï¼š{str(e)}")
        sys.exit(1)


def init_temp_csv(temp_file=TEMP_CSV):
    """åˆå§‹åŒ–ä¸´æ—¶CSVæ–‡ä»¶"""
    if not os.path.exists(temp_file):
        with open(temp_file, "w", newline="", encoding="utf-8") as f:
            csv_columns = get_csv_columns()
            writer = csv.DictWriter(f, fieldnames=csv_columns, restval="")
            writer.writeheader()
    return temp_file


def get_csv_columns():
    """å®šä¹‰CSVè¾“å‡ºå­—æ®µï¼ˆä¸api_crawler.pyä¿æŒä¸€è‡´ï¼‰"""
    return [
        # åŸºç¡€å®šä½ä¿¡æ¯
        "original_row_num", "url", "crawl_time", "crawl_status", "error_msg",
        # APIæ ¸å¿ƒä¿¡æ¯
        "api", "package", "language",
        # APIå˜æ›´ä¿¡æ¯
        "deprecated_in", "removed_in", "replaced_by", "change_type", "reason",
        # æ¥æºä¿¡æ¯
        "source"
    ]


def write_result_to_csv(result, csv_file, lock):
    """çº¿ç¨‹å®‰å…¨çš„CSVå†™å…¥å‡½æ•°"""
    with lock:
        with open(csv_file, "a", newline="", encoding="utf-8") as f:
            csv_columns = get_csv_columns()
            writer = csv.DictWriter(f, fieldnames=csv_columns, restval="")
            # è¿‡æ»¤æ‰ä¸åœ¨å­—æ®µåˆ—è¡¨ä¸­çš„é”®
            filtered_result = {k: result.get(k, "") for k in csv_columns}
            writer.writerow(filtered_result)


# -------------------------- æ‰¹é‡çˆ¬å–ä¸»é€»è¾‘ --------------------------
def batch_crawl_large_scale(input_csv, output_csv, temp_csv):
    # 1. åˆå§‹åŒ–
    all_urls = load_urls_from_csv(input_csv, temp_csv)
    if not all_urls:
        print("ğŸ‰ æ‰€æœ‰URLå·²çˆ¬å–å®Œæˆï¼Œæ— éœ€ç»§ç»­æ‰§è¡Œ")
        if os.path.exists(temp_csv) and not os.path.exists(output_csv):
            os.rename(temp_csv, output_csv)
        sys.exit(0)

    init_temp_csv(temp_csv)

    # 2. åˆå§‹åŒ–è¿›åº¦ç»Ÿè®¡
    total_to_crawl = len(all_urls)
    completed_count = 0
    success_count = 0
    fail_count = 0
    start_time = datetime.now()

    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å–ï¼š{start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š é…ç½®ï¼šçº¿ç¨‹æ•°={MAX_WORKERS}ï¼Œé‡è¯•æ¬¡æ•°={MAX_RETRIES}")
    print(f"ğŸ¤– æ™ºèƒ½APIè¯†åˆ«ï¼š{'å¯ç”¨' if AZURE_API_KEY else 'ç¦ç”¨'}")
    print(f"â³ é¢„è®¡è€—æ—¶ï¼š{total_to_crawl / MAX_WORKERS * 2:.1f} ç§’ï¼ˆä¼°ç®—ï¼‰\n")

    # 3. åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = EnhancedAPICrawler()

    # 4. å¤šçº¿ç¨‹æ‰¹é‡çˆ¬å–
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_tasks = {
            executor.submit(crawler.crawl_single_api, url_info["url"], url_info["original_row_num"]):
            url_info for url_info in all_urls
        }

        # å®æ—¶å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_tasks):
            url_info = future_tasks[future]
            url = url_info["url"]
            completed_count += 1

            try:
                result = future.result(timeout=60)
                write_result_to_csv(result, temp_csv, csv_lock)

                if result["crawl_status"] == "success":
                    success_count += 1
                    print(f"âœ… [{completed_count}/{total_to_crawl}] æˆåŠŸï¼š{url} â†’ {result.get('api', 'N/A')}")
                else:
                    fail_count += 1
                    print(f"âŒ [{completed_count}/{total_to_crawl}] å¤±è´¥ï¼š{url}ï¼ˆ{result['error_msg'][:50]}...ï¼‰")

                # æ‰¹é‡è¿›åº¦æ±‡æ€»
                if completed_count % BATCH_SIZE == 0 or completed_count == total_to_crawl:
                    elapsed_time = (datetime.now() - start_time).total_seconds()
                    avg_time_per_url = elapsed_time / completed_count if completed_count > 0 else 0
                    remaining_time = avg_time_per_url * (total_to_crawl - completed_count)
                    print(f"\n è¿›åº¦æ±‡æ€»ï¼šå·²å®Œæˆ{completed_count}/{total_to_crawl}ï¼ˆæˆåŠŸ{success_count}ï¼Œå¤±è´¥{fail_count}ï¼‰")
                    print(f"â±  å·²è€—æ—¶ï¼š{elapsed_time:.1f}ç§’ï¼Œé¢„è®¡å‰©ä½™ï¼š{remaining_time:.1f}ç§’\n")

            except Exception as e:
                fail_count += 1
                error_result = {
                    "original_row_num": url_info["original_row_num"],
                    "url": url,
                    "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "crawl_status": "failed",
                    "error_msg": f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}"
                }
                write_result_to_csv(error_result, temp_csv, csv_lock)
                print(f"âŒ [{completed_count}/{total_to_crawl}] å¼‚å¸¸ï¼š{url}ï¼ˆ{str(e)[:50]}...ï¼‰")

    # 5. å®Œæˆï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    end_time = datetime.now()
    total_elapsed = (end_time - start_time).total_seconds()

    # å°†ä¸´æ—¶æ–‡ä»¶é‡å‘½åä¸ºæœ€ç»ˆè¾“å‡ºæ–‡ä»¶
    if os.path.exists(temp_csv):
        if os.path.exists(output_csv):
            os.remove(output_csv)
        os.rename(temp_csv, output_csv)
        print(f" ä¸´æ—¶æ–‡ä»¶å·²åˆå¹¶ä¸ºæœ€ç»ˆç»“æœï¼š{os.path.abspath(output_csv)}")

    # æ‰“å°æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print(" å¢å¼ºç‰ˆæ‰¹é‡çˆ¬å–ä»»åŠ¡å®Œæˆ")
    print("=" * 60)
    print(f" æ€»ç»Ÿè®¡ï¼š")
    print(f"   - è¾“å…¥URLæ€»æ•°ï¼š{len(all_urls) + success_count + fail_count}")
    print(f"   - å¾…çˆ¬URLæ•°ï¼š{total_to_crawl}")
    print(f"   - æˆåŠŸæ•°ï¼š{success_count}")
    print(f"   - å¤±è´¥æ•°ï¼š{fail_count}")
    print(f"   - æˆåŠŸç‡ï¼š{success_count / total_to_crawl * 100:.1f}%" if total_to_crawl > 0 else "0%")
    print(f"â±  è€—æ—¶ï¼š{total_elapsed // 60:.0f}åˆ†{total_elapsed % 60:.1f}ç§’")
    print(f" æ™ºèƒ½è¯†åˆ«ï¼š{'GPT-4oå¢å¼º' if crawler.use_gpt else 'åŸºç¡€è§£æ'}")
    print(f" ç»“æœæ–‡ä»¶ï¼š{os.path.abspath(output_file)}")
    print("=" * 60)


# -------------------------- æ‰§è¡Œå…¥å£ --------------------------
if __name__ == "__main__":
    # 1. æ·»åŠ é¡¹ç›®è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.append(current_dir)

    # æ·»åŠ å¿…è¦çš„å¯¼å…¥
    import re

    # 2. æ£€æŸ¥ä¾èµ–æ–‡ä»¶
    if not os.path.exists(INPUT_CSV):
        print(f"âŒ è¾“å…¥CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{INPUT_CSV}")
        print("ğŸ’¡ æç¤ºï¼šè¯·ä¿®æ”¹INPUT_CSVå˜é‡ä¸ºæ­£ç¡®çš„æ–‡ä»¶è·¯å¾„")
        sys.exit(1)

    # 3. å¯åŠ¨å¢å¼ºç‰ˆæ‰¹é‡çˆ¬å–
    batch_crawl_large_scale(
        input_csv=INPUT_CSV,
        output_csv=OUTPUT_CSV,
        temp_csv=TEMP_CSV
    )